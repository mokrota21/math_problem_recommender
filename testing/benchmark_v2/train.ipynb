{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "733e0bef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "text",
         "rawType": "object",
         "type": "string"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "dfd65fe3-b465-43cf-810f-22ab24975e15",
       "rows": [
        [
         "Chinese Remainder Theorem",
         "['Solving $x \\\\equiv 2 \\\\mod 3$ and $x \\\\equiv 3 \\\\mod 5$ gives $x \\\\equiv 8 \\\\mod 15$.', 'If $a \\\\equiv b \\\\mod m$ and $a \\\\equiv b \\\\mod n$ with $\\\\gcd(m,n)=1$, then $a \\\\equiv b \\\\mod mn$.', 'The system $x \\\\equiv 1 \\\\mod 4$, $x \\\\equiv 2 \\\\mod 7$ has a unique solution modulo 28.', 'Using CRT to combine residues modulo 3 and 5 to find solutions modulo 15.', 'For pairwise coprime moduli, the system of congruences has a unique solution modulo the product.', 'CRT implies $\\\\mathbb{Z}/mn\\\\mathbb{Z} \\\\cong \\\\mathbb{Z}/m\\\\mathbb{Z} \\\\times \\\\mathbb{Z}/n\\\\mathbb{Z}$ when $\\\\gcd(m,n)=1$.', 'To solve $x^2 \\\\equiv 1 \\\\mod 24$, solve modulo 3 and 8, then combine solutions.', 'CRT allows reconstruction of integers from their residues modulo prime powers.', 'The number of solutions to $x \\\\equiv a \\\\mod m$ and $x \\\\equiv b \\\\mod n$ is 1 if $\\\\gcd(m,n)=1$.', 'Using CRT to find a number congruent to 1 mod 2, 2 mod 3, and 3 mod 5.']"
        ],
        [
         "Diophantine Equations",
         "['Solving $3x + 5y = 1$ using the extended Euclidean algorithm.', 'Find all integer solutions to $x^2 + y^2 = z^2$ (Pythagorean triples).', 'The equation $x^n + y^n = z^n$ has no nontrivial solutions for $n > 2$ (Fermat’s Last Theorem).', 'Using substitution to solve $2x + 3y = 7$ for integers $x, y$.', 'The Pell equation $x^2 - 2y^2 = 1$ has infinitely many solutions.', 'Parametrizing solutions to $x^2 - y^2 = N$ as $(x+y)(x-y) = N$.', 'For $ax + by = c$, solutions exist iff $\\\\gcd(a,b)$ divides $c$.', 'The equation $x^2 + y^2 = 4z + 3$ has no integer solutions.', 'Finding all primes $p$ such that $p = x^2 + 2y^2$.', 'The only solution to $x^3 + y^3 = z^3$ in positive integers is trivial.']"
        ],
        [
         "Divisibility",
         "['From (1) it follows that $A(x_{1}+y_{1})=p^{k-n\\\\alpha}$ , so, since $x_{1}+y_{1}>1$ and $A>1$ , both of these numbers are divisible by $p$ ; moreover, $x_{1}+y_{1}=p^{\\\\beta}$ for some natural number $\\\\beta$ .', 'Since $A$ is divisible by $p$ and $x_{1}$ is relatively prime to $p$ , it follows that $n$ is divisible by $p$ .', 'In fact, every integer that is not divisible by 10 divides some element of $X$', 'Therefore $m(10^{(d+1)\\\\varphi(f k)}-1)/(10^{d+1}-1)$ is divisible by $2^{p}k$', 'Note that 2 divides $2^{\\\\pi}+2$ for all $n$ .', 'Because $m\\\\equiv-1$ (mod 3), there exists an odd prime $p$ such that $p\\\\equiv-1$ (mod 3) and $p|m$ .', \"If a prime $p$ divides $ab$, then $p$ divides $a$ or $p$ divides $b$ (Euclid's lemma).\", 'For integers $a,b$, if $a$ divides $b$ and $b$ divides $a$, then $a = \\\\pm b$.', 'Any integer $n > 1$ has a prime divisor.', 'Using the divisibility rule for 9: a number is divisible by 9 if the sum of its digits is divisible by 9.']"
        ],
        [
         "Euler’s Theorem",
         "['By Euler’s Theorem, $10^{\\\\varphi(f k)}\\\\equiv1$ (mod $f k$ )', 'To compute $3^{100} \\\\mod 14$, note $\\\\varphi(14)=6$, so $3^6 \\\\equiv 1 \\\\mod 14$, hence $3^{100} \\\\equiv 3^4 \\\\mod 14$.', 'If $a \\\\equiv b \\\\mod n$ and $\\\\gcd(a,n)=1$, then $a^k \\\\equiv b^k \\\\mod n$ for $k = \\\\varphi(n)$.', 'Using Euler’s theorem to find the inverse of $a$ modulo $n$ as $a^{\\\\varphi(n)-1} \\\\mod n$.', 'Since 7 and 10 are coprime, $10^{\\\\varphi(7)} \\\\equiv 10^6 \\\\equiv 1 \\\\mod 7$.', 'Applying Euler’s theorem to simplify $2^{103} \\\\mod 13$, where $\\\\varphi(13)=12$, so $2^{12} \\\\equiv 1 \\\\mod 13$.', 'Given $\\\\gcd(m,n)=1$, $m^{\\\\varphi(n)} + n^{\\\\varphi(m)} \\\\equiv 1 + 1 \\\\equiv 2 \\\\mod mn$.', 'In solving $x \\\\equiv a^b \\\\mod m$, reduce the exponent $b$ modulo $\\\\varphi(m)$ if $\\\\gcd(a,m)=1$.', 'Euler’s theorem implies the period of $1/7$ in decimal repeats every 6 digits because $\\\\varphi(7)=6$.', 'For $\\\\gcd(a,n)=1$, $a^{\\\\varphi(n)} \\\\equiv 1 \\\\mod n$ by Euler’s theorem.']"
        ],
        [
         "Extremal Principles",
         "['If $x\\\\geq3$ , $y\\\\geq3$ , $z\\\\geq3$ then $x y z\\\\geq3y z$ , $x y z\\\\geq3x z$ , $x y z\\\\geq3x y$ .', 'Let $a_{2}$ be the maximum integer such that $2^{a_{2}}|a$.', 'Find $a,b$ such that $\\\\operatorname*{min}\\\\{a_{5}a+b_{5}b,a_{2}a+b_{2}b\\\\}=98$ and $a b$ is minimal.', 'If $n$ is odd, let $d$ be the smallest odd prime that does not divide $n$ .', 'The minimal positive integer $k$ such that $k!$ is divisible by $n$ is the maximal prime power in $n$.', 'By the pigeonhole principle, among any five integers, two will have a difference divisible by 4.', 'The maximum value of $\\\\gcd(n, n+1)$ is 1, since consecutive integers are coprime.', 'For the smallest $n$ where $2^n \\\\equiv 1 \\\\mod 7$, trial shows $n=3$.', 'The minimal solution to $x^2 - 2y^2 = 1$ is $(3,2)$ by testing small integers.', 'In any set of 10 integers, there exist two whose difference is divisible by 9.']"
        ],
        [
         "Fermat’s Little Theorem",
         "['The formula in our problem shows that the sum of the quotients obtained when $k^{p}{-}k$ is divided by $p$ (Fermat’s Little Theorem) is congruent to $\\\\frac{p+1}{2}$ modulo $p$.', 'By Fermat’s Little Theorem, $a^{p-1} \\\\equiv 1 \\\\mod p$ for $\\\\gcd(a,p)=1$.', 'To prove $p$ divides $1^{p-1} + 2^{p-1} + \\\\dots + (p-1)^{p-1} \\\\equiv -1 \\\\mod p$.', 'If $p$ is prime, then $(p-1)! \\\\equiv -1 \\\\mod p$ by Wilson’s theorem, related to Fermat’s theorem.', 'Using Fermat’s theorem: $3^{10} \\\\equiv 1 \\\\mod 11$, so $3^{100} \\\\equiv 1^{10} \\\\equiv 1 \\\\mod 11$.', 'For prime $p$, $a^p \\\\equiv a \\\\mod p$ holds for all integers $a$.', 'Since $p \\\\nmid a$, $a^{p-2} \\\\equiv a^{-1} \\\\mod p$ by Fermat’s Little Theorem.', 'Fermat’s theorem implies $2^{6} \\\\equiv 1 \\\\mod 7$ because $7$ is prime.', 'To solve $5x \\\\equiv 1 \\\\mod 7$, multiply both sides by $5^{5} \\\\equiv 3 \\\\mod 7$ (since $5^{6} \\\\equiv 1$).', 'If $p$ is prime, $1 + 2 + \\\\dots + (p-1) \\\\equiv 0 \\\\mod p$ by Fermat’s theorem.']"
        ],
        [
         "Modular Arithmetics",
         "['Assume that we have a prime $p$ such that $p|2^{n}+1$ and $p\\\\equiv-1$ (mod 8)', 'If $n$ is even, then $p\\\\equiv3$ (mod 4) and $\\\\left(\\\\frac{-1}{p}\\\\right)=1$ , a contradiction.', 'If $n$ is odd, then $\\\\left(\\\\frac{-2}{p}\\\\right)=1$ and we get $(-1)^{\\\\frac{p^{2}-1}{8}}(-1)^{\\\\frac{p-1}{2}}=1$ , again a contradiction.', 'Since $|S|=2^{n}>k$ , we can find two elements $a<b$ of $S$ which are congruent modulo $k$ , and $b-a$ only has the digits 8, 9, 0, 1 in its decimal representation.', 'Then we can choose $a_{2k}$ so that $x+a_{2k}10^{2k}\\\\equiv0$ (mod $4^{k+1}$ ) since $10^{2k}\\\\equiv0\\\\pmod{4^{k}}$ )', 'Also, 11 divides $2^{n}+2$ if and only if $n\\\\equiv6$ (mod 10), and 43 divides $2^{n}+2$ if and only if $n\\\\equiv8$ (mod 14).', 'Observe that for any integer $x$ we have $x^{4}=16k$ or $x^{4}=16k+1$ for some $k$.', 'Using the Chinese Remainder Theorem to solve $x \\\\equiv 2 \\\\mod 3$ and $x \\\\equiv 3 \\\\mod 5$ yields $x \\\\equiv 8 \\\\mod 15$.', 'If $a \\\\equiv b \\\\mod m$ and $a \\\\equiv b \\\\mod n$ with $\\\\gcd(m,n)=1$, then $a \\\\equiv b \\\\mod mn$.', 'Wilson’s theorem states that $(p-1)! \\\\equiv -1 \\\\mod p$ for prime $p$.']"
        ],
        [
         "Pigeonhole Principle",
         "['Let $S$ be the set of nonnegative integers less than $10^{n}$ whose decimal digits are all 0 or 1. Since $|S|=2^{n}>k$ , we can find two elements $a<b$ of $S$ which are congruent modulo $k$ , and $b-a$ only has the digits 8, 9, 0, 1 in its decimal representation.', 'It suffices to take 25 such numbers.', 'Observe that from any 2001 consecutive natural numbers, at least one is a term of the sequence.', 'Among any 13 integers, there exist two whose difference is divisible by 12.', 'In any set of 52 integers, two must have the same remainder modulo 51.', 'For seven distinct real numbers, two will satisfy $0 < \\\\frac{a - b}{1 + ab} < 1$.', 'Given 10 points in a unit square, some two are at most $\\\\sqrt{2}/3$ apart.', 'Any subset of 55 numbers from 1 to 100 contains two numbers differing by 9.', 'In a sequence of $n^2+1$ distinct numbers, there exists a monotonic subsequence of length $n+1$.', 'Among 367 people, at least two share a birthday by the pigeonhole principle.']"
        ],
        [
         "Prime Numbers",
         "['Assume that we have a prime $p$ such that $p|2^{n}+1$ and $p\\\\equiv-1$ (mod 8)', 'Then $x=m x_{1}$ , $y=m y_{1}$ and by virtue of the given equation, $m^{n}(x_{1}^{n}+y_{1}^{n})=p^{k}$ , and so $m=p^{\\\\alpha}$ for some nonnegative integer $\\\\alpha$ .', 'By the condition $p>2$ , it follows that at least one of $x_{1},y_{1}$ is greater than $1$ , so since $n>1$ , $A>1$ .', 'From (1) it follows that $A(x_{1}+y_{1})=p^{k-n\\\\alpha}$ , so, since $x_{1}+y_{1}>1$ and $A>1$ , both of these numbers are divisible by $p$ ; moreover, $x_{1}+y_{1}=p^{\\\\beta}$ for some natural number $\\\\beta$ .', 'Since $A$ is divisible by $p$ and $x_{1}$ is relatively prime to $p$ , it follows that $n$ is divisible by $p$ .', 'If $n$ is odd, let $d$ be the smallest odd prime that does not divide $n$ .', 'The number $d n$ contains exactly one more prime factor than $n$ .', 'Because $m\\\\equiv-1$ (mod 3), there exists an odd prime $p$ such that $p\\\\equiv-1$ (mod 3) and $p|m$ .', 'To each number, associate the triple $(x_{2},x_{3},x_{5})$ recording the parity of the exponents of 2, 3, and 5 in its prime factorization.', 'The values are $k=(p+1)^{2}/4$ for $p$ odd (and none for $p=2$ ).']"
        ],
        [
         "Quadratic Residues",
         "['If $n$ is even, then $p\\\\equiv3$ (mod 4) and $\\\\left(\\\\frac{-1}{p}\\\\right)=1$ , a contradiction.', 'If $n$ is odd, then $\\\\left(\\\\frac{-2}{p}\\\\right)=1$ and we get $(-1)^{\\\\frac{p^{2}-1}{8}}(-1)^{\\\\frac{p-1}{2}}=1$ , again a contradiction.', 'It remains to see whether $\\\\left(\\\\frac{3}{q}\\\\right)=-1$ .', 'Note that $(p/3)=(-1/3)=-1$ .', 'Using the Legendre symbol: $\\\\left(\\\\frac{5}{p}\\\\right) = \\\\left(\\\\frac{p}{5}\\\\right)$ by quadratic reciprocity.', 'For prime $p \\\\equiv 1 \\\\mod 4$, $-1$ is a quadratic residue modulo $p$.', 'The equation $x^2 \\\\equiv -1 \\\\mod p$ has solutions if and only if $p \\\\equiv 1 \\\\mod 4$.', 'If $p \\\\equiv 3 \\\\mod 4$, then $\\\\left(\\\\frac{-3}{p}\\\\right) = \\\\left(\\\\frac{p}{3}\\\\right)$.', 'By Euler’s criterion, $a^{(p-1)/2} \\\\equiv \\\\left(\\\\frac{a}{p}\\\\right) \\\\mod p$.', 'The number of quadratic residues modulo a prime $p$ is $(p-1)/2$.']"
        ]
       ],
       "shape": {
        "columns": 1,
        "rows": 10
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Chinese Remainder Theorem</th>\n",
       "      <td>[Solving $x \\equiv 2 \\mod 3$ and $x \\equiv 3 \\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Diophantine Equations</th>\n",
       "      <td>[Solving $3x + 5y = 1$ using the extended Eucl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Divisibility</th>\n",
       "      <td>[From (1) it follows that $A(x_{1}+y_{1})=p^{k...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Euler’s Theorem</th>\n",
       "      <td>[By Euler’s Theorem, $10^{\\varphi(f k)}\\equiv1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Extremal Principles</th>\n",
       "      <td>[If $x\\geq3$ , $y\\geq3$ , $z\\geq3$ then $x y z...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fermat’s Little Theorem</th>\n",
       "      <td>[The formula in our problem shows that the sum...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Modular Arithmetics</th>\n",
       "      <td>[Assume that we have a prime $p$ such that $p|...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pigeonhole Principle</th>\n",
       "      <td>[Let $S$ be the set of nonnegative integers le...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Prime Numbers</th>\n",
       "      <td>[Assume that we have a prime $p$ such that $p|...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Quadratic Residues</th>\n",
       "      <td>[If $n$ is even, then $p\\equiv3$ (mod 4) and $...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                        text\n",
       "Chinese Remainder Theorem  [Solving $x \\equiv 2 \\mod 3$ and $x \\equiv 3 \\...\n",
       "Diophantine Equations      [Solving $3x + 5y = 1$ using the extended Eucl...\n",
       "Divisibility               [From (1) it follows that $A(x_{1}+y_{1})=p^{k...\n",
       "Euler’s Theorem            [By Euler’s Theorem, $10^{\\varphi(f k)}\\equiv1...\n",
       "Extremal Principles        [If $x\\geq3$ , $y\\geq3$ , $z\\geq3$ then $x y z...\n",
       "Fermat’s Little Theorem    [The formula in our problem shows that the sum...\n",
       "Modular Arithmetics        [Assume that we have a prime $p$ such that $p|...\n",
       "Pigeonhole Principle       [Let $S$ be the set of nonnegative integers le...\n",
       "Prime Numbers              [Assume that we have a prime $p$ such that $p|...\n",
       "Quadratic Residues         [If $n$ is even, then $p\\equiv3$ (mod 4) and $..."
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "df = pd.read_json(os.path.abspath(\"..\\\\..\\\\benchmark\\\\benchmark_v2\\\\benchmark_v2.json\"))\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5d3c985",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['label', 'text', '__index_level_0__'],\n",
       "        num_rows: 80\n",
       "    })\n",
       "    eval: Dataset({\n",
       "        features: ['label', 'text', '__index_level_0__'],\n",
       "        num_rows: 20\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = df.explode(\"text\").reset_index().rename({\"index\": \"label\"}, axis=1)\n",
    "\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42, shuffle=True)\n",
    "\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "test_dataset = Dataset.from_pandas(test_df)\n",
    "\n",
    "dataset = DatasetDict({\n",
    "    \"train\": train_dataset,\n",
    "    \"eval\": test_dataset\n",
    "})\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d55bfe08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('tbs17/MathBERT', output_hidden_states=True)\n",
    "model = BertModel.from_pretrained(\"tbs17/MathBERT\")\n",
    "device = torch.device(\"cuda\")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "741c1895",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def collate_fn(batch):\n",
    "    texts = [item['text'] for item in batch]\n",
    "    encoding = tokenizer(texts, padding=True, truncation=True, return_tensors='pt')\n",
    "    return {'input_ids': encoding['input_ids'], 'attention_mask': encoding['attention_mask']}\n",
    "\n",
    "train_dataloader = DataLoader(dataset['train'], batch_size=16, shuffle=True, collate_fn=collate_fn)\n",
    "eval_dataloader = DataLoader(dataset['eval'], batch_size=16, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f445d99",
   "metadata": {},
   "source": [
    "## GPT guide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "38851adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('tbs17/MathBERT', output_hidden_states=True)\n",
    "bert = BertModel.from_pretrained(\"tbs17/MathBERT\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "bert = bert.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2eecc69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = {\n",
    "    0: [\"Use distributive law\", \"Apply a(b + c) = ab + ac\"],\n",
    "    1: [\"Subtract both sides\", \"Eliminate by subtraction\"],\n",
    "    2: [\"Combine like terms\", \"Group similar variables\"],\n",
    "    # ... up to cluster 9\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b96b4bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_pairs(clusters):\n",
    "    pairs = []\n",
    "    labels = []\n",
    "\n",
    "    # Positive pairs (same cluster)\n",
    "    for cluster in clusters.values():\n",
    "        for i in range(len(cluster)):\n",
    "            for j in range(i + 1, len(cluster)):\n",
    "                pairs.append((cluster[i], cluster[j]))\n",
    "                labels.append(1)\n",
    "\n",
    "    # Negative pairs (different clusters)\n",
    "    cluster_keys = list(clusters.keys())\n",
    "    for _ in range(len(pairs)):\n",
    "        c1, c2 = random.sample(cluster_keys, 2)\n",
    "        t1 = random.choice(clusters[c1])\n",
    "        t2 = random.choice(clusters[c2])\n",
    "        pairs.append((t1, t2))\n",
    "        labels.append(0)\n",
    "    \n",
    "    return pairs, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cf8e6b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(text, tokenizer):\n",
    "    tokens = tokenizer(text, return_tensors='pt', truncation=True, padding=True)\n",
    "    tokens = {k: v.to(device) for k, v in tokens.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = bert(**tokens)\n",
    "    return outputs.last_hidden_state[:, 0, :]  # [CLS] token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a33546d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContrastiveModel(nn.Module):\n",
    "    def __init__(self, encoder, tokenizer):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.tokenizer = tokenizer\n",
    "        self.projection = nn.Linear(encoder.config.hidden_size, 128)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        z1 = self.projection(encode(x1, self.tokenizer))\n",
    "        z2 = self.projection(encode(x2, self.tokenizer))\n",
    "        return z1, z2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4a946a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, clusters, epochs=2):\n",
    "    pairs, labels = make_pairs(clusters)\n",
    "    criterion = nn.CosineEmbeddingLoss(margin=0.5)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=2e-5)\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for (t1, t2), label in zip(pairs, labels):\n",
    "            z1, z2 = model(t1, t2)\n",
    "            target = torch.tensor([1.0 if label == 1 else -1.0], device=device)\n",
    "            loss = criterion(z1, z2, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            total_loss += loss.item()\n",
    "        print(f\"Epoch {epoch+1}, Loss: {total_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "357d99ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 1.5187\n",
      "Epoch 2, Loss: 1.3850\n"
     ]
    }
   ],
   "source": [
    "model = ContrastiveModel(bert, tokenizer).to(device)\n",
    "train(model, clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a7f518eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score\n",
    "from collections import Counter\n",
    "\n",
    "def get_embeddings(model, tokenizer, texts, device, batch_size=32):\n",
    "    \"\"\"\n",
    "    Returns a (N, D) array of projection-head embeddings for a list of texts.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    embeds = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            batch = texts[i:i+batch_size]\n",
    "            toks  = tokenizer(batch,\n",
    "                              return_tensors='pt',\n",
    "                              padding=True,\n",
    "                              truncation=True,\n",
    "                              max_length=128).to(device)\n",
    "            # get [CLS] hidden state\n",
    "            h = model.encoder(**toks).last_hidden_state[:, 0, :]    # (B, H)\n",
    "            z = model.projection(h)                                # (B, D)\n",
    "            embeds.append(z.cpu().numpy())\n",
    "    return np.vstack(embeds)  # (N, D)\n",
    "\n",
    "def test_clustering_performance(model, tokenizer, clusters, device):\n",
    "    \"\"\"\n",
    "    clusters: dict[int, List[str]] mapping true cluster id → list of texts\n",
    "    Returns: dict with ARI, NMI, Purity\n",
    "    \"\"\"\n",
    "    # 1) flatten texts & labels\n",
    "    texts, true_labels = [], []\n",
    "    for cid, docs in clusters.items():\n",
    "        texts.extend(docs)\n",
    "        true_labels.extend([cid] * len(docs))\n",
    "    true_labels = np.array(true_labels)\n",
    "\n",
    "    # 2) get embeddings\n",
    "    X = get_embeddings(model, tokenizer, texts, device)\n",
    "\n",
    "    # 3) K-Means\n",
    "    kmeans = KMeans(n_clusters=len(clusters), random_state=42)\n",
    "    pred_labels = kmeans.fit_predict(X)\n",
    "\n",
    "    # 4) metrics\n",
    "    ari = adjusted_rand_score(true_labels, pred_labels)\n",
    "    nmi = normalized_mutual_info_score(true_labels, pred_labels)\n",
    "\n",
    "    # 5) purity\n",
    "    # for each predicted cluster, find the most common true label\n",
    "    total = len(texts)\n",
    "    correct = 0\n",
    "    for pid in np.unique(pred_labels):\n",
    "        mask = pred_labels == pid\n",
    "        true_in_cluster = true_labels[mask]\n",
    "        most_common = Counter(true_in_cluster).most_common(1)[0][1]\n",
    "        correct += most_common\n",
    "    purity = correct / total\n",
    "\n",
    "    return {\n",
    "        \"ARI\": ari,\n",
    "        \"NMI\": nmi,\n",
    "        \"Purity\": purity\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "335c9896",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score\n",
    "from sklearn.decomposition import PCA\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def evaluate_and_visualize(model, tokenizer, clusters, device,\n",
    "                           batch_size=32, pca_components=2,\n",
    "                           random_state=42):\n",
    "    \"\"\"\n",
    "    clusters: dict[int, List[str]] mapping true cluster → list of texts\n",
    "    Returns:\n",
    "      metrics: dict with 'ARI', 'NMI', 'Purity'\n",
    "      fig: matplotlib.figure.Figure showing 2D scatter of embeddings\n",
    "    \"\"\"\n",
    "    # 1) flatten texts & labels\n",
    "    texts, true_labels = [], []\n",
    "    for cid, docs in clusters.items():\n",
    "        texts.extend(docs)\n",
    "        true_labels.extend([cid] * len(docs))\n",
    "    true_labels = np.array(true_labels)\n",
    "\n",
    "    # 2) get embeddings (using your get_embeddings helper)\n",
    "    X = get_embeddings(model, tokenizer, texts, device, batch_size)  # shape (N, D)\n",
    "\n",
    "    # 3) K-Means clustering\n",
    "    kmeans = KMeans(n_clusters=len(clusters), random_state=random_state)\n",
    "    pred_labels = kmeans.fit_predict(X)\n",
    "\n",
    "    # 4) compute metrics\n",
    "    ari = adjusted_rand_score(true_labels, pred_labels)\n",
    "    nmi = normalized_mutual_info_score(true_labels, pred_labels)\n",
    "    # purity\n",
    "    total = len(texts)\n",
    "    correct = 0\n",
    "    for pid in np.unique(pred_labels):\n",
    "        mask = pred_labels == pid\n",
    "        most_common = Counter(true_labels[mask]).most_common(1)[0][1]\n",
    "        correct += most_common\n",
    "    purity = correct / total\n",
    "\n",
    "    metrics = {\"ARI\": ari, \"NMI\": nmi, \"Purity\": purity}\n",
    "\n",
    "    # 5) PCA to 2D\n",
    "    pca = PCA(n_components=pca_components, random_state=random_state)\n",
    "    coords = pca.fit_transform(X)  # shape (N, 2)\n",
    "\n",
    "    # 6) plot\n",
    "    fig, ax = plt.subplots(figsize=(6,6))\n",
    "    scatter = ax.scatter(coords[:,0], coords[:,1],\n",
    "                         c=true_labels,\n",
    "                         cmap='tab10',\n",
    "                         alpha=0.7,\n",
    "                         edgecolors='k')\n",
    "    ax.set_title(\"2D PCA of Strategy Embeddings\")\n",
    "    ax.set_xlabel(\"PC1\")\n",
    "    ax.set_ylabel(\"PC2\")\n",
    "    legend1 = ax.legend(*scatter.legend_elements(),\n",
    "                        title=\"True Cluster\",\n",
    "                        loc=\"upper right\",\n",
    "                        bbox_to_anchor=(1.25, 1))\n",
    "    ax.add_artist(legend1)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    return metrics, fig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6362f6c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_interactive(model, tokenizer, clusters, device='cpu', batch_size=32, pca_components=2):\n",
    "    \"\"\"\n",
    "    Computes embeddings for each text in clusters on CPU, reduces to 2D, and\n",
    "    returns an interactive Plotly scatter plot with hover information.\n",
    "    \"\"\"\n",
    "    # 1) flatten texts & labels\n",
    "    texts, true_labels = [], []\n",
    "    for cid, docs in clusters.items():\n",
    "        texts.extend(docs)\n",
    "        true_labels.extend([cid] * len(docs))\n",
    "    true_labels = np.array(true_labels)\n",
    "    \n",
    "    # 2) get embeddings (reuse existing helper, forcing CPU)\n",
    "    X = get_embeddings(model, tokenizer, texts, device='cpu', batch_size=batch_size)\n",
    "    \n",
    "    # 3) PCA to 2D\n",
    "    pca = PCA(n_components=pca_components, random_state=42)\n",
    "    coords = pca.fit_transform(X)  # shape (N, 2)\n",
    "    \n",
    "    # 4) build dataframe for Plotly\n",
    "    df = pd.DataFrame({\n",
    "        'PC1': coords[:, 0],\n",
    "        'PC2': coords[:, 1],\n",
    "        'cluster': true_labels.astype(str),\n",
    "        'text': texts\n",
    "    })\n",
    "    \n",
    "    # 5) create interactive plot\n",
    "    fig = px.scatter(\n",
    "        df,\n",
    "        x='PC1',\n",
    "        y='PC2',\n",
    "        color='cluster',\n",
    "        hover_data=['text'],\n",
    "        title=\"Interactive 2D PCA of Strategy Embeddings\"\n",
    "    )\n",
    "    fig.update_layout(\n",
    "        xaxis_title=\"Principal Component 1\",\n",
    "        yaxis_title=\"Principal Component 2\",\n",
    "        legend_title=\"True Cluster\"\n",
    "    )\n",
    "    fig.show()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9d708422",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ARI:    0.4444\n",
      "NMI:    0.7397\n",
      "Purity: 0.8333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mokrota\\AppData\\Local\\Temp\\ipykernel_41620\\1963297986.py:5: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n",
      "  fig.show()   # or plt.show()\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "metrics, fig = evaluate_and_visualize(model, tokenizer, clusters, device)\n",
    "print(f\"ARI:    {metrics['ARI']:.4f}\")\n",
    "print(f\"NMI:    {metrics['NMI']:.4f}\")\n",
    "print(f\"Purity: {metrics['Purity']:.4f}\")\n",
    "fig.show()   # or plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e95434",
   "metadata": {},
   "source": [
    "## Try for my model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "31f027af",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('tbs17/MathBERT', output_hidden_states=True)\n",
    "bert = BertModel.from_pretrained(\"tbs17/MathBERT\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "bert = bert.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b2c794a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = {}\n",
    "for row in df.iterrows():\n",
    "    label = row[1]['label']\n",
    "    text = row[1]['text']\n",
    "    clusters[label] = clusters.get(label, [])\n",
    "    clusters[label].append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c3db9008",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 200.0334\n",
      "Epoch 2, Loss: 212.3310\n",
      "Epoch 3, Loss: 213.2060\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[34], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m ContrastiveModel(bert, tokenizer)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m----> 2\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclusters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[10], line 10\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, clusters, epochs)\u001b[0m\n\u001b[0;32m      8\u001b[0m total_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m (t1, t2), label \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(pairs, labels):\n\u001b[1;32m---> 10\u001b[0m     z1, z2 \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m     target \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([\u001b[38;5;241m1.0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m label \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1.0\u001b[39m], device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[0;32m     12\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(z1, z2, target)\n",
      "File \u001b[1;32mc:\\Users\\mokrota\\Documents\\GitHub\\math_problem_recommender\\math_problem_recommender\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\mokrota\\Documents\\GitHub\\math_problem_recommender\\math_problem_recommender\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[9], line 9\u001b[0m, in \u001b[0;36mContrastiveModel.forward\u001b[1;34m(self, x1, x2)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x1, x2):\n\u001b[1;32m----> 9\u001b[0m     z1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprojection(\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     10\u001b[0m     z2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprojection(encode(x2, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer))\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m z1, z2\n",
      "Cell \u001b[1;32mIn[8], line 3\u001b[0m, in \u001b[0;36mencode\u001b[1;34m(text, tokenizer)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mencode\u001b[39m(text, tokenizer):\n\u001b[0;32m      2\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m tokenizer(text, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m'\u001b[39m, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m----> 3\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m {k: \u001b[43mv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m tokens\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m      5\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m bert(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtokens)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = ContrastiveModel(bert, tokenizer).to(device)\n",
    "train(model, clusters, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29467acc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
