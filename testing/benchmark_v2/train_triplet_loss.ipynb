{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e28d8cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "text",
         "rawType": "object",
         "type": "string"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "736d97cc-26e2-479d-ad35-b4e46e916656",
       "rows": [
        [
         "Chinese Remainder Theorem",
         "['Solving $x \\\\equiv 2 \\\\mod 3$ and $x \\\\equiv 3 \\\\mod 5$ gives $x \\\\equiv 8 \\\\mod 15$.', 'If $a \\\\equiv b \\\\mod m$ and $a \\\\equiv b \\\\mod n$ with $\\\\gcd(m,n)=1$, then $a \\\\equiv b \\\\mod mn$.', 'The system $x \\\\equiv 1 \\\\mod 4$, $x \\\\equiv 2 \\\\mod 7$ has a unique solution modulo 28.', 'Using CRT to combine residues modulo 3 and 5 to find solutions modulo 15.', 'For pairwise coprime moduli, the system of congruences has a unique solution modulo the product.', 'CRT implies $\\\\mathbb{Z}/mn\\\\mathbb{Z} \\\\cong \\\\mathbb{Z}/m\\\\mathbb{Z} \\\\times \\\\mathbb{Z}/n\\\\mathbb{Z}$ when $\\\\gcd(m,n)=1$.', 'To solve $x^2 \\\\equiv 1 \\\\mod 24$, solve modulo 3 and 8, then combine solutions.', 'CRT allows reconstruction of integers from their residues modulo prime powers.', 'The number of solutions to $x \\\\equiv a \\\\mod m$ and $x \\\\equiv b \\\\mod n$ is 1 if $\\\\gcd(m,n)=1$.', 'Using CRT to find a number congruent to 1 mod 2, 2 mod 3, and 3 mod 5.']"
        ],
        [
         "Diophantine Equations",
         "['Solving $3x + 5y = 1$ using the extended Euclidean algorithm.', 'Find all integer solutions to $x^2 + y^2 = z^2$ (Pythagorean triples).', 'The equation $x^n + y^n = z^n$ has no nontrivial solutions for $n > 2$ (Fermat’s Last Theorem).', 'Using substitution to solve $2x + 3y = 7$ for integers $x, y$.', 'The Pell equation $x^2 - 2y^2 = 1$ has infinitely many solutions.', 'Parametrizing solutions to $x^2 - y^2 = N$ as $(x+y)(x-y) = N$.', 'For $ax + by = c$, solutions exist iff $\\\\gcd(a,b)$ divides $c$.', 'The equation $x^2 + y^2 = 4z + 3$ has no integer solutions.', 'Finding all primes $p$ such that $p = x^2 + 2y^2$.', 'The only solution to $x^3 + y^3 = z^3$ in positive integers is trivial.']"
        ],
        [
         "Divisibility",
         "['From (1) it follows that $A(x_{1}+y_{1})=p^{k-n\\\\alpha}$ , so, since $x_{1}+y_{1}>1$ and $A>1$ , both of these numbers are divisible by $p$ ; moreover, $x_{1}+y_{1}=p^{\\\\beta}$ for some natural number $\\\\beta$ .', 'Since $A$ is divisible by $p$ and $x_{1}$ is relatively prime to $p$ , it follows that $n$ is divisible by $p$ .', 'In fact, every integer that is not divisible by 10 divides some element of $X$', 'Therefore $m(10^{(d+1)\\\\varphi(f k)}-1)/(10^{d+1}-1)$ is divisible by $2^{p}k$', 'Note that 2 divides $2^{\\\\pi}+2$ for all $n$ .', 'Because $m\\\\equiv-1$ (mod 3), there exists an odd prime $p$ such that $p\\\\equiv-1$ (mod 3) and $p|m$ .', \"If a prime $p$ divides $ab$, then $p$ divides $a$ or $p$ divides $b$ (Euclid's lemma).\", 'For integers $a,b$, if $a$ divides $b$ and $b$ divides $a$, then $a = \\\\pm b$.', 'Any integer $n > 1$ has a prime divisor.', 'Using the divisibility rule for 9: a number is divisible by 9 if the sum of its digits is divisible by 9.']"
        ],
        [
         "Euler’s Theorem",
         "['By Euler’s Theorem, $10^{\\\\varphi(f k)}\\\\equiv1$ (mod $f k$ )', 'To compute $3^{100} \\\\mod 14$, note $\\\\varphi(14)=6$, so $3^6 \\\\equiv 1 \\\\mod 14$, hence $3^{100} \\\\equiv 3^4 \\\\mod 14$.', 'If $a \\\\equiv b \\\\mod n$ and $\\\\gcd(a,n)=1$, then $a^k \\\\equiv b^k \\\\mod n$ for $k = \\\\varphi(n)$.', 'Using Euler’s theorem to find the inverse of $a$ modulo $n$ as $a^{\\\\varphi(n)-1} \\\\mod n$.', 'Since 7 and 10 are coprime, $10^{\\\\varphi(7)} \\\\equiv 10^6 \\\\equiv 1 \\\\mod 7$.', 'Applying Euler’s theorem to simplify $2^{103} \\\\mod 13$, where $\\\\varphi(13)=12$, so $2^{12} \\\\equiv 1 \\\\mod 13$.', 'Given $\\\\gcd(m,n)=1$, $m^{\\\\varphi(n)} + n^{\\\\varphi(m)} \\\\equiv 1 + 1 \\\\equiv 2 \\\\mod mn$.', 'In solving $x \\\\equiv a^b \\\\mod m$, reduce the exponent $b$ modulo $\\\\varphi(m)$ if $\\\\gcd(a,m)=1$.', 'Euler’s theorem implies the period of $1/7$ in decimal repeats every 6 digits because $\\\\varphi(7)=6$.', 'For $\\\\gcd(a,n)=1$, $a^{\\\\varphi(n)} \\\\equiv 1 \\\\mod n$ by Euler’s theorem.']"
        ],
        [
         "Extremal Principles",
         "['If $x\\\\geq3$ , $y\\\\geq3$ , $z\\\\geq3$ then $x y z\\\\geq3y z$ , $x y z\\\\geq3x z$ , $x y z\\\\geq3x y$ .', 'Let $a_{2}$ be the maximum integer such that $2^{a_{2}}|a$.', 'Find $a,b$ such that $\\\\operatorname*{min}\\\\{a_{5}a+b_{5}b,a_{2}a+b_{2}b\\\\}=98$ and $a b$ is minimal.', 'If $n$ is odd, let $d$ be the smallest odd prime that does not divide $n$ .', 'The minimal positive integer $k$ such that $k!$ is divisible by $n$ is the maximal prime power in $n$.', 'By the pigeonhole principle, among any five integers, two will have a difference divisible by 4.', 'The maximum value of $\\\\gcd(n, n+1)$ is 1, since consecutive integers are coprime.', 'For the smallest $n$ where $2^n \\\\equiv 1 \\\\mod 7$, trial shows $n=3$.', 'The minimal solution to $x^2 - 2y^2 = 1$ is $(3,2)$ by testing small integers.', 'In any set of 10 integers, there exist two whose difference is divisible by 9.']"
        ],
        [
         "Fermat’s Little Theorem",
         "['The formula in our problem shows that the sum of the quotients obtained when $k^{p}{-}k$ is divided by $p$ (Fermat’s Little Theorem) is congruent to $\\\\frac{p+1}{2}$ modulo $p$.', 'By Fermat’s Little Theorem, $a^{p-1} \\\\equiv 1 \\\\mod p$ for $\\\\gcd(a,p)=1$.', 'To prove $p$ divides $1^{p-1} + 2^{p-1} + \\\\dots + (p-1)^{p-1} \\\\equiv -1 \\\\mod p$.', 'If $p$ is prime, then $(p-1)! \\\\equiv -1 \\\\mod p$ by Wilson’s theorem, related to Fermat’s theorem.', 'Using Fermat’s theorem: $3^{10} \\\\equiv 1 \\\\mod 11$, so $3^{100} \\\\equiv 1^{10} \\\\equiv 1 \\\\mod 11$.', 'For prime $p$, $a^p \\\\equiv a \\\\mod p$ holds for all integers $a$.', 'Since $p \\\\nmid a$, $a^{p-2} \\\\equiv a^{-1} \\\\mod p$ by Fermat’s Little Theorem.', 'Fermat’s theorem implies $2^{6} \\\\equiv 1 \\\\mod 7$ because $7$ is prime.', 'To solve $5x \\\\equiv 1 \\\\mod 7$, multiply both sides by $5^{5} \\\\equiv 3 \\\\mod 7$ (since $5^{6} \\\\equiv 1$).', 'If $p$ is prime, $1 + 2 + \\\\dots + (p-1) \\\\equiv 0 \\\\mod p$ by Fermat’s theorem.']"
        ],
        [
         "Modular Arithmetics",
         "['Assume that we have a prime $p$ such that $p|2^{n}+1$ and $p\\\\equiv-1$ (mod 8)', 'If $n$ is even, then $p\\\\equiv3$ (mod 4) and $\\\\left(\\\\frac{-1}{p}\\\\right)=1$ , a contradiction.', 'If $n$ is odd, then $\\\\left(\\\\frac{-2}{p}\\\\right)=1$ and we get $(-1)^{\\\\frac{p^{2}-1}{8}}(-1)^{\\\\frac{p-1}{2}}=1$ , again a contradiction.', 'Since $|S|=2^{n}>k$ , we can find two elements $a<b$ of $S$ which are congruent modulo $k$ , and $b-a$ only has the digits 8, 9, 0, 1 in its decimal representation.', 'Then we can choose $a_{2k}$ so that $x+a_{2k}10^{2k}\\\\equiv0$ (mod $4^{k+1}$ ) since $10^{2k}\\\\equiv0\\\\pmod{4^{k}}$ )', 'Also, 11 divides $2^{n}+2$ if and only if $n\\\\equiv6$ (mod 10), and 43 divides $2^{n}+2$ if and only if $n\\\\equiv8$ (mod 14).', 'Observe that for any integer $x$ we have $x^{4}=16k$ or $x^{4}=16k+1$ for some $k$.', 'Using the Chinese Remainder Theorem to solve $x \\\\equiv 2 \\\\mod 3$ and $x \\\\equiv 3 \\\\mod 5$ yields $x \\\\equiv 8 \\\\mod 15$.', 'If $a \\\\equiv b \\\\mod m$ and $a \\\\equiv b \\\\mod n$ with $\\\\gcd(m,n)=1$, then $a \\\\equiv b \\\\mod mn$.', 'Wilson’s theorem states that $(p-1)! \\\\equiv -1 \\\\mod p$ for prime $p$.']"
        ],
        [
         "Pigeonhole Principle",
         "['Let $S$ be the set of nonnegative integers less than $10^{n}$ whose decimal digits are all 0 or 1. Since $|S|=2^{n}>k$ , we can find two elements $a<b$ of $S$ which are congruent modulo $k$ , and $b-a$ only has the digits 8, 9, 0, 1 in its decimal representation.', 'It suffices to take 25 such numbers.', 'Observe that from any 2001 consecutive natural numbers, at least one is a term of the sequence.', 'Among any 13 integers, there exist two whose difference is divisible by 12.', 'In any set of 52 integers, two must have the same remainder modulo 51.', 'For seven distinct real numbers, two will satisfy $0 < \\\\frac{a - b}{1 + ab} < 1$.', 'Given 10 points in a unit square, some two are at most $\\\\sqrt{2}/3$ apart.', 'Any subset of 55 numbers from 1 to 100 contains two numbers differing by 9.', 'In a sequence of $n^2+1$ distinct numbers, there exists a monotonic subsequence of length $n+1$.', 'Among 367 people, at least two share a birthday by the pigeonhole principle.']"
        ],
        [
         "Prime Numbers",
         "['Assume that we have a prime $p$ such that $p|2^{n}+1$ and $p\\\\equiv-1$ (mod 8)', 'Then $x=m x_{1}$ , $y=m y_{1}$ and by virtue of the given equation, $m^{n}(x_{1}^{n}+y_{1}^{n})=p^{k}$ , and so $m=p^{\\\\alpha}$ for some nonnegative integer $\\\\alpha$ .', 'By the condition $p>2$ , it follows that at least one of $x_{1},y_{1}$ is greater than $1$ , so since $n>1$ , $A>1$ .', 'From (1) it follows that $A(x_{1}+y_{1})=p^{k-n\\\\alpha}$ , so, since $x_{1}+y_{1}>1$ and $A>1$ , both of these numbers are divisible by $p$ ; moreover, $x_{1}+y_{1}=p^{\\\\beta}$ for some natural number $\\\\beta$ .', 'Since $A$ is divisible by $p$ and $x_{1}$ is relatively prime to $p$ , it follows that $n$ is divisible by $p$ .', 'If $n$ is odd, let $d$ be the smallest odd prime that does not divide $n$ .', 'The number $d n$ contains exactly one more prime factor than $n$ .', 'Because $m\\\\equiv-1$ (mod 3), there exists an odd prime $p$ such that $p\\\\equiv-1$ (mod 3) and $p|m$ .', 'To each number, associate the triple $(x_{2},x_{3},x_{5})$ recording the parity of the exponents of 2, 3, and 5 in its prime factorization.', 'The values are $k=(p+1)^{2}/4$ for $p$ odd (and none for $p=2$ ).']"
        ],
        [
         "Quadratic Residues",
         "['If $n$ is even, then $p\\\\equiv3$ (mod 4) and $\\\\left(\\\\frac{-1}{p}\\\\right)=1$ , a contradiction.', 'If $n$ is odd, then $\\\\left(\\\\frac{-2}{p}\\\\right)=1$ and we get $(-1)^{\\\\frac{p^{2}-1}{8}}(-1)^{\\\\frac{p-1}{2}}=1$ , again a contradiction.', 'It remains to see whether $\\\\left(\\\\frac{3}{q}\\\\right)=-1$ .', 'Note that $(p/3)=(-1/3)=-1$ .', 'Using the Legendre symbol: $\\\\left(\\\\frac{5}{p}\\\\right) = \\\\left(\\\\frac{p}{5}\\\\right)$ by quadratic reciprocity.', 'For prime $p \\\\equiv 1 \\\\mod 4$, $-1$ is a quadratic residue modulo $p$.', 'The equation $x^2 \\\\equiv -1 \\\\mod p$ has solutions if and only if $p \\\\equiv 1 \\\\mod 4$.', 'If $p \\\\equiv 3 \\\\mod 4$, then $\\\\left(\\\\frac{-3}{p}\\\\right) = \\\\left(\\\\frac{p}{3}\\\\right)$.', 'By Euler’s criterion, $a^{(p-1)/2} \\\\equiv \\\\left(\\\\frac{a}{p}\\\\right) \\\\mod p$.', 'The number of quadratic residues modulo a prime $p$ is $(p-1)/2$.']"
        ]
       ],
       "shape": {
        "columns": 1,
        "rows": 10
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Chinese Remainder Theorem</th>\n",
       "      <td>[Solving $x \\equiv 2 \\mod 3$ and $x \\equiv 3 \\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Diophantine Equations</th>\n",
       "      <td>[Solving $3x + 5y = 1$ using the extended Eucl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Divisibility</th>\n",
       "      <td>[From (1) it follows that $A(x_{1}+y_{1})=p^{k...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Euler’s Theorem</th>\n",
       "      <td>[By Euler’s Theorem, $10^{\\varphi(f k)}\\equiv1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Extremal Principles</th>\n",
       "      <td>[If $x\\geq3$ , $y\\geq3$ , $z\\geq3$ then $x y z...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fermat’s Little Theorem</th>\n",
       "      <td>[The formula in our problem shows that the sum...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Modular Arithmetics</th>\n",
       "      <td>[Assume that we have a prime $p$ such that $p|...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pigeonhole Principle</th>\n",
       "      <td>[Let $S$ be the set of nonnegative integers le...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Prime Numbers</th>\n",
       "      <td>[Assume that we have a prime $p$ such that $p|...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Quadratic Residues</th>\n",
       "      <td>[If $n$ is even, then $p\\equiv3$ (mod 4) and $...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                        text\n",
       "Chinese Remainder Theorem  [Solving $x \\equiv 2 \\mod 3$ and $x \\equiv 3 \\...\n",
       "Diophantine Equations      [Solving $3x + 5y = 1$ using the extended Eucl...\n",
       "Divisibility               [From (1) it follows that $A(x_{1}+y_{1})=p^{k...\n",
       "Euler’s Theorem            [By Euler’s Theorem, $10^{\\varphi(f k)}\\equiv1...\n",
       "Extremal Principles        [If $x\\geq3$ , $y\\geq3$ , $z\\geq3$ then $x y z...\n",
       "Fermat’s Little Theorem    [The formula in our problem shows that the sum...\n",
       "Modular Arithmetics        [Assume that we have a prime $p$ such that $p|...\n",
       "Pigeonhole Principle       [Let $S$ be the set of nonnegative integers le...\n",
       "Prime Numbers              [Assume that we have a prime $p$ such that $p|...\n",
       "Quadratic Residues         [If $n$ is even, then $p\\equiv3$ (mod 4) and $..."
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "df = pd.read_json(os.path.abspath(\"..\\\\..\\\\benchmark\\\\benchmark_v2\\\\benchmark_v2.json\"))\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ad191b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = df.explode(\"text\").reset_index().rename({\"index\": \"label\"}, axis=1)\n",
    "labels = df['label'].unique()\n",
    "label_maping = {labels[i]: i for i in range(len(labels))}\n",
    "df['label'] = df.apply(lambda x: label_maping[x['label']], axis=1)\n",
    "\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09d4a992",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('tbs17/MathBERT', output_hidden_states=True)\n",
    "model = BertModel.from_pretrained(\"tbs17/MathBERT\")\n",
    "device = torch.device(\"cuda\")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b881d7b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(text, tokenizer):\n",
    "    tokens = tokenizer(text, padding=\"max_length\", truncation=True, return_tensors=\"pt\", max_length=256)\n",
    "    tokens = {k: v.to(device) for k, v in tokens.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**tokens)\n",
    "    return outputs.last_hidden_state[:, 0, :] # [cls] tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1a54d78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(batch):\n",
    "    return tokenizer(batch['text'], padding='max_length', truncation=True, return_tensors=\"pt\", max_length=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c58457c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_metric_learning.miners import TripletMarginMiner\n",
    "miner = TripletMarginMiner(margin=0.2, type_of_triplets=\"semihard\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4188923d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c98f88b9c214e35b4b2bb49fa867d99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/80 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label': [5, 8, 2, 4, 6, 1, 4, 9, 0, 7], 'text': ['For prime $p$, $a^p \\\\equiv a \\\\mod p$ holds for all integers $a$.', 'To each number, associate the triple $(x_{2},x_{3},x_{5})$ recording the parity of the exponents of 2, 3, and 5 in its prime factorization.', \"If a prime $p$ divides $ab$, then $p$ divides $a$ or $p$ divides $b$ (Euclid's lemma).\", 'Find $a,b$ such that $\\\\operatorname*{min}\\\\{a_{5}a+b_{5}b,a_{2}a+b_{2}b\\\\}=98$ and $a b$ is minimal.', 'Wilson’s theorem states that $(p-1)! \\\\equiv -1 \\\\mod p$ for prime $p$.', 'Parametrizing solutions to $x^2 - y^2 = N$ as $(x+y)(x-y) = N$.', 'If $x\\\\geq3$ , $y\\\\geq3$ , $z\\\\geq3$ then $x y z\\\\geq3y z$ , $x y z\\\\geq3x z$ , $x y z\\\\geq3x y$ .', 'The equation $x^2 \\\\equiv -1 \\\\mod p$ has solutions if and only if $p \\\\equiv 1 \\\\mod 4$.', 'Using CRT to find a number congruent to 1 mod 2, 2 mod 3, and 3 mod 5.', 'Observe that from any 2001 consecutive natural numbers, at least one is a term of the sequence.'], '__index_level_0__': [55, 88, 26, 42, 69, 15, 40, 96, 9, 72], 'input_ids': [[[101, 2005, 3539, 1002, 1052, 1002, 1010, 1002, 1037, 1034, 1052, 1032, 1041, 15549, 2615, 1037, 1032, 16913, 1052, 1002, 4324, 2005, 2035, 24028, 1002, 1037, 1002, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], [[101, 2000, 2169, 2193, 1010, 5482, 1996, 6420, 1002, 1006, 1060, 1035, 1063, 1016, 1065, 1010, 1060, 1035, 1063, 1017, 1065, 1010, 1060, 1035, 1063, 1019, 1065, 1007, 1002, 3405, 1996, 11968, 3012, 1997, 1996, 16258, 21576, 2015, 1997, 1016, 1010, 1017, 1010, 1998, 1019, 1999, 2049, 3539, 5387, 3989, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], [[101, 2065, 1037, 3539, 1002, 1052, 1002, 20487, 1002, 11113, 1002, 1010, 2059, 1002, 1052, 1002, 20487, 1002, 1037, 1002, 2030, 1002, 1052, 1002, 20487, 1002, 1038, 1002, 1006, 7327, 20464, 3593, 1005, 1055, 3393, 14760, 1007, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], [[101, 2424, 1002, 1037, 1010, 1038, 1002, 2107, 2008, 1002, 1032, 6872, 18442, 1008, 1063, 8117, 1065, 1032, 1063, 1037, 1035, 1063, 1019, 1065, 1037, 1009, 1038, 1035, 1063, 1019, 1065, 1038, 1010, 1037, 1035, 1063, 1016, 1065, 1037, 1009, 1038, 1035, 1063, 1016, 1065, 1038, 1032, 1065, 1027, 5818, 1002, 1998, 1002, 1037, 1038, 1002, 2003, 10124, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], [[101, 4267, 1521, 1055, 9872, 2163, 2008, 1002, 1006, 1052, 1011, 1015, 1007, 999, 1032, 1041, 15549, 2615, 1011, 1015, 1032, 16913, 1052, 1002, 2005, 3539, 1002, 1052, 1002, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], [[101, 11498, 11368, 21885, 2075, 7300, 2000, 1002, 1060, 1034, 1016, 1011, 1061, 1034, 1016, 1027, 1050, 1002, 2004, 1002, 1006, 1060, 1009, 1061, 1007, 1006, 1060, 1011, 1061, 1007, 1027, 1050, 1002, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], [[101, 2065, 1002, 1060, 1032, 16216, 4160, 2509, 1002, 1010, 1002, 1061, 1032, 16216, 4160, 2509, 1002, 1010, 1002, 1062, 1032, 16216, 4160, 2509, 1002, 2059, 1002, 1060, 1061, 1062, 1032, 16216, 4160, 2509, 2100, 1062, 1002, 1010, 1002, 1060, 1061, 1062, 1032, 16216, 4160, 2509, 2595, 1062, 1002, 1010, 1002, 1060, 1061, 1062, 1032, 16216, 4160, 2509, 2595, 1061, 1002, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], [[101, 1996, 8522, 1002, 1060, 1034, 1016, 1032, 1041, 15549, 2615, 1011, 1015, 1032, 16913, 1052, 1002, 2038, 7300, 2065, 1998, 2069, 2065, 1002, 1052, 1032, 1041, 15549, 2615, 1015, 1032, 16913, 1018, 1002, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], [[101, 2478, 13675, 2102, 2000, 2424, 1037, 2193, 26478, 6820, 4765, 2000, 1015, 16913, 1016, 1010, 1016, 16913, 1017, 1010, 1998, 1017, 16913, 1019, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], [[101, 11949, 2008, 2013, 2151, 2541, 5486, 3019, 3616, 1010, 2012, 2560, 2028, 2003, 1037, 2744, 1997, 1996, 5537, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]], 'token_type_ids': [[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]], 'attention_mask': [[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]]}\n",
      "['input_ids', 'token_type_ids', 'attention_mask']\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "only integer tensors of a single element can be converted to an index",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 38\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m dataloader:\n\u001b[0;32m     35\u001b[0m     \u001b[38;5;66;03m# toks_batch is dict of batched tensors\u001b[39;00m\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;28mprint\u001b[39m(tokenizer\u001b[38;5;241m.\u001b[39mmodel_input_names)\n\u001b[0;32m     37\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m---> 38\u001b[0m         k: \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlong\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     39\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m batch\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m     40\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mmodel_input_names\n\u001b[0;32m     41\u001b[0m     }\n\u001b[0;32m     42\u001b[0m     labels \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     44\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m model(inputs)  \u001b[38;5;66;03m# (B, D)\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: only integer tensors of a single element can be converted to an index"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "from transformers import AutoTokenizer, BertModel\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from pytorch_metric_learning.miners import TripletMarginMiner\n",
    "from pytorch_metric_learning.losses import TripletMarginLoss\n",
    "from pytorch_metric_learning.samplers import MPerClassSampler\n",
    "import torch\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "# Assuming you have train_df and a tokenize function defined\n",
    "# Example tokenize function (adjust based on your needs):\n",
    "def tokenize(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=128)\n",
    "\n",
    "class MathBertEmbeddingModel(nn.Module):\n",
    "    def __init__(self, projection_dim=128):\n",
    "        super().__init__()\n",
    "        self.encoder = BertModel.from_pretrained(\"tbs17/MathBERT\")\n",
    "        self.projection = nn.Linear(self.encoder.config.hidden_size, projection_dim)\n",
    "\n",
    "    def forward(self, toks):\n",
    "        out = self.encoder(**toks).last_hidden_state[:, 0, :]  # CLS token\n",
    "        return self.projection(out)  # embedding (B, D)\n",
    "\n",
    "# Load your data and tokenize it\n",
    "train_df = pd.DataFrame({'text': [\"some math text 1\", \"another math text 2\", \"yet more math 1\", \"example 2\"], 'label': [0, 1, 0, 1]}) # Example DataFrame\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"tbs17/MathBERT\")\n",
    "dataset = Dataset.from_pandas(train_df)\n",
    "dataset = dataset.map(tokenize, batched=True)\n",
    "dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label']) # Ensure correct format for DataLoader\n",
    "\n",
    "# Create the DataLoader with a sampler (optional but recommended)\n",
    "sampler = MPerClassSampler(labels=train_df['label'].values, m=2, batch_size=4) # Adjust m and batch_size\n",
    "dataloader = DataLoader(dataset, batch_sampler=sampler)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "embedding_model = MathBertEmbeddingModel(projection_dim=128).to(device)\n",
    "optimizer = torch.optim.AdamW(embedding_model.parameters(), lr=2e-5)\n",
    "\n",
    "loss_fn = TripletMarginLoss(margin=0.3)\n",
    "miner = TripletMarginMiner(margin=0.3, type_of_triplets=\"semihard\")\n",
    "\n",
    "embedding_model.train()\n",
    "for epoch in range(5):\n",
    "    total_loss = 0\n",
    "    for batch in dataloader:\n",
    "        inputs = {k: v.to(device) for k, v in batch.items() if k in tokenizer.model_input_names}\n",
    "        labels = batch['label'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        embeddings = embedding_model(inputs)  # (B, projection_dim)\n",
    "        triplets = miner(embeddings, labels)\n",
    "\n",
    "        if triplets[0].nelement() == 0:\n",
    "            continue  # skip if miner returns no triplets\n",
    "\n",
    "        loss = loss_fn(embeddings, labels, triplets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Loss: {total_loss:.4f}\")\n",
    "\n",
    "embedding_model.eval() # Set model to evaluation mode after training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "746d744e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "889df3d4b58a4d6e93281b754eb2ea54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'to'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 47\u001b[0m\n\u001b[0;32m     45\u001b[0m total_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m dataloader:\n\u001b[1;32m---> 47\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m {k: \u001b[43mv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m(device) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m batch\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mmodel_input_names}\n\u001b[0;32m     48\u001b[0m     labels \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     50\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'to'"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "from transformers import AutoModel, BertModel\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from pytorch_metric_learning.miners import TripletMarginMiner\n",
    "from pytorch_metric_learning.losses import TripletMarginLoss\n",
    "from pytorch_metric_learning.samplers import MPerClassSampler\n",
    "import torch\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "# Assuming you have train_df and a tokenize function defined\n",
    "# Example tokenize function (adjust based on your needs):\n",
    "def tokenize(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=128)\n",
    "\n",
    "class MathBertEmbeddingModel(nn.Module):\n",
    "    def __init__(self, projection_dim=128):\n",
    "        super().__init__()\n",
    "        self.encoder = BertModel.from_pretrained(\"tbs17/MathBERT\")\n",
    "        self.projection = nn.Linear(self.encoder.config.hidden_size, projection_dim)\n",
    "\n",
    "    def forward(self, toks):\n",
    "        out = self.encoder(**toks).last_hidden_state[:, 0, :]  # CLS token\n",
    "        return self.projection(out)  # embedding (B, D)\n",
    "\n",
    "# Load your data and tokenize it\n",
    "train_df = pd.DataFrame({'text': [\"some math text 1\", \"another math text 2\", \"yet more math 1\", \"example 2\"], 'label': [0, 1, 0, 1]}) # Example DataFrame\n",
    "tokenizer = BertTokenizer.from_pretrained(\"tbs17/MathBERT\")\n",
    "dataset = Dataset.from_pandas(train_df)\n",
    "dataset = dataset.map(tokenize, batched=True)\n",
    "# dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label']) # Ensure correct format for DataLoader\n",
    "\n",
    "# Create the DataLoader with a sampler (optional but recommended)\n",
    "sampler = MPerClassSampler(labels=train_df['label'].values, m=2, batch_size=4) # Adjust m and batch_size\n",
    "dataloader = DataLoader(dataset, batch_sampler=sampler)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "embedding_model = MathBertEmbeddingModel(projection_dim=128).to(device)\n",
    "optimizer = torch.optim.AdamW(embedding_model.parameters(), lr=2e-5)\n",
    "\n",
    "loss_fn = TripletMarginLoss(margin=0.3)\n",
    "miner = TripletMarginMiner(margin=0.3, type_of_triplets=\"semihard\")\n",
    "\n",
    "embedding_model.train()\n",
    "for epoch in range(5):\n",
    "    total_loss = 0\n",
    "    for batch in dataloader:\n",
    "        inputs = {k: v.to(device) for k, v in batch.items() if k in tokenizer.model_input_names}\n",
    "        labels = batch['label'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        embeddings = embedding_model(inputs)  # (B, projection_dim)\n",
    "        triplets = miner(embeddings, labels)\n",
    "\n",
    "        if triplets[0].nelement() == 0:\n",
    "            continue  # skip if miner returns no triplets\n",
    "\n",
    "        loss = loss_fn(embeddings, labels, triplets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Loss: {total_loss:.4f}\")\n",
    "\n",
    "embedding_model.eval() # Set model to evaluation mode after training"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
